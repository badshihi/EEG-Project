{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Utilities\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Keras Package\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Flatten, LSTM, Embedding, Reshape, GRU, Input, RNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "#Numpy\n",
    "import numpy as np\n",
    "#Load Data\n",
    "import h5py, glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#Timing\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "def import_data(filename):\n",
    "    #Removes nan trial from imported data\n",
    "    def remove_nan(X):\n",
    "        idx = 0\n",
    "        idx_nan = []\n",
    "        for trial in X:\n",
    "            if (np.isnan(trial).any()):\n",
    "                print('Trial %d has nan' % idx)\n",
    "                idx_nan.append(idx)\n",
    "            idx += 1\n",
    "        return np.delete(X,idx_nan,0), np.delete(y,idx_nan,0)\n",
    "\n",
    "    #Load data\n",
    "    A01T = h5py.File(filename, 'r')\n",
    "    X = np.copy(A01T['image'])\n",
    "    X = X[:,0:22,:] #remove EOG lines\n",
    "\n",
    "    #769-left hand; 770-right hand; 771-both feet; 772-tongue\n",
    "    y = np.copy(A01T['type'])\n",
    "    y = y[0,0:X.shape[0]:1]\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    \n",
    "    #Data Preprocess\n",
    "    X, y = remove_nan(X) #Remove nans\n",
    "    X = np.transpose(X,(0,2,1))\n",
    "    X = np.expand_dims(X,3) #Expand dimension\n",
    "\n",
    "    #Convert y to one-hot label\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)\n",
    "    y = np_utils.to_categorical(encoded_y)\n",
    "    num_classes = y.shape[1]\n",
    "\n",
    "    #Check whole dimensions\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    return X, y, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Trial 56 has nan\n",
      "(287, 1000, 22, 1)\n",
      "(287, 4)\n",
      "1\n",
      "Trial 237 has nan\n",
      "Trial 284 has nan\n",
      "(286, 1000, 22, 1)\n",
      "(286, 4)\n",
      "2\n",
      "Trial 113 has nan\n",
      "Trial 249 has nan\n",
      "(286, 1000, 22, 1)\n",
      "(286, 4)\n",
      "3\n",
      "Trial 144 has nan\n",
      "Trial 145 has nan\n",
      "Trial 146 has nan\n",
      "Trial 179 has nan\n",
      "(284, 1000, 22, 1)\n",
      "(284, 4)\n",
      "4\n",
      "Trial 6 has nan\n",
      "Trial 28 has nan\n",
      "Trial 57 has nan\n",
      "Trial 101 has nan\n",
      "Trial 220 has nan\n",
      "Trial 225 has nan\n",
      "(282, 1000, 22, 1)\n",
      "(282, 4)\n",
      "5\n",
      "Trial 97 has nan\n",
      "Trial 115 has nan\n",
      "Trial 140 has nan\n",
      "(285, 1000, 22, 1)\n",
      "(285, 4)\n",
      "6\n",
      "(288, 1000, 22, 1)\n",
      "(288, 4)\n",
      "7\n",
      "Trial 58 has nan\n",
      "Trial 81 has nan\n",
      "Trial 124 has nan\n",
      "Trial 151 has nan\n",
      "Trial 178 has nan\n",
      "Trial 275 has nan\n",
      "(282, 1000, 22, 1)\n",
      "(282, 4)\n",
      "8\n",
      "Trial 22 has nan\n",
      "Trial 61 has nan\n",
      "Trial 92 has nan\n",
      "Trial 93 has nan\n",
      "Trial 159 has nan\n",
      "Trial 202 has nan\n",
      "Trial 204 has nan\n",
      "Trial 218 has nan\n",
      "Trial 239 has nan\n",
      "Trial 250 has nan\n",
      "(278, 1000, 22, 1)\n",
      "(278, 4)\n"
     ]
    }
   ],
   "source": [
    "#Load files\n",
    "files = glob.glob('project_datasets/*.mat')\n",
    "subjects = -1\n",
    "X = {}\n",
    "y = {}\n",
    "for file in files:\n",
    "    subjects += 1\n",
    "    print(subjects)\n",
    "    X[str(subjects)], y[str(subjects)], num_classes = import_data(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to concatenate = 0.845703\n",
      "\n",
      "(2558, 1000, 22, 1)\n",
      "(2302, 1000, 22, 1)\n",
      "(128, 1000, 22, 1)\n",
      "(128, 1000, 22, 1)\n",
      "\n",
      "(2558, 4)\n",
      "(2302, 4)\n",
      "(128, 4)\n",
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all data (for v2_bulk)\n",
    "t0 = time()\n",
    "X_net = X[str(0)];\n",
    "y_net = y[str(0)];\n",
    "for i in np.arange(1,len(X)):\n",
    "    X_net = np.concatenate((X_net,X[str(i)]),axis=0)\n",
    "    y_net = np.concatenate((y_net,y[str(i)]),axis=0)\n",
    "t1 = time()\n",
    "total = t1 - t0\n",
    "print(\"Time to concatenate = %f\" % total)\n",
    "print()\n",
    "\n",
    "# Creating Validation and Testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_net, y_net, test_size = 5/100, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 5/95, random_state=0)\n",
    "print(X_net.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print()\n",
    "print(y_net.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Concatenate all data (for v1_bulk)\n",
    "# idx = np.arange(len(X))\n",
    "# idx_train = idx[0:-2]\n",
    "# idx_val = idx[-2]\n",
    "# idx_test = idx[-1]\n",
    "# X_train, y_train = X[str(idx_train[0])], y[str(idx_train[0])]\n",
    "# X_val, y_val = X[str(idx_val)], y[str(idx_val)]\n",
    "# X_test, y_test = X[str(idx_test)], y[str(idx_test)]\n",
    "# for i in idx_train[1:]:\n",
    "#     X_train = np.concatenate((X_train,X[str(i)]),axis=0)\n",
    "#     y_train = np.concatenate((y_train,y[str(i)]),axis=0)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(X_test.shape)\n",
    "# print()\n",
    "# print(y_train.shape)\n",
    "# print(y_val.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Deep CNN\n",
    "# Conv2D: (layer name, filter length, number of filters, stride size)\n",
    "# GRU layers: (layer name, number of filters)\n",
    "def create_rnn():\n",
    "    activation = 'elu'\n",
    "    input_shape = Input(shape = X['1'][0].shape)\n",
    "    init = 'glorot_uniform'\n",
    "\n",
    "    #Block 1\n",
    "#     cnn_1 = Conv2D(25, (10,1), kernel_initializer=init)(input_shape)\n",
    "#     cnn_1 = Conv2D(25, (1,22), kernel_initializer=init)(cnn_1)\n",
    "#     cnn_1 = BatchNormalization()(cnn_1)\n",
    "#     cnn_1 = Activation(activation)(cnn_1)\n",
    "#     cnn_1 = MaxPooling2D(pool_size=(3, 1), strides=3)(cnn_1)\n",
    "#     cnn_1 = Dropout(0.5)(cnn_1)\n",
    "    \n",
    "    #Inception 1\n",
    "    tower_11 = Conv2D(50, (5,22), strides=(5,22), padding='same', kernel_initializer=init)(input_shape)\n",
    "    tower_12 = Conv2D(50, (10,22), strides=(5,22), padding='same', kernel_initializer=init)(input_shape)\n",
    "    tower_13 = Conv2D(50, (20,22), strides=(5,22), padding='same', kernel_initializer=init)(input_shape)\n",
    "    inception_output_1 = keras.layers.concatenate([tower_11, tower_12, tower_13], axis = 3)\n",
    "    inception_output_1 = BatchNormalization()(inception_output_1)\n",
    "    inception_output_1 = Activation(activation)(inception_output_1)\n",
    "    inception_output_1 = Dropout(0.5)(inception_output_1)\n",
    "    \n",
    "    #Inception 2\n",
    "    tower_21 = Conv2D(50, (5,1), strides=5, padding='same', kernel_initializer=init)(inception_output_1)\n",
    "    tower_22 = Conv2D(50, (10,1), strides=5, padding='same', kernel_initializer=init)(inception_output_1)\n",
    "    tower_23 = Conv2D(50, (20,1), strides=5, padding='same', kernel_initializer=init)(inception_output_1)\n",
    "    inception_output_2 = keras.layers.concatenate([tower_21, tower_22, tower_23], axis = 3)\n",
    "    inception_output_2 = BatchNormalization()(inception_output_2)\n",
    "    inception_output_2 = Activation(activation)(inception_output_2)\n",
    "    inception_output_2 = Dropout(0.5)(inception_output_2)\n",
    "    \n",
    "    #Inception 3\n",
    "    tower_31 = Conv2D(50, (5,1), strides=5, padding='same', kernel_initializer=init)(inception_output_2)\n",
    "    tower_32 = Conv2D(50, (10,1), strides=5, padding='same', kernel_initializer=init)(inception_output_2)\n",
    "    tower_33 = Conv2D(50, (20,1), strides=5, padding='same', kernel_initializer=init)(inception_output_2)\n",
    "    inception_output_3 = keras.layers.concatenate([tower_31, tower_32, tower_33], axis = 3)\n",
    "    inception_output_3 = BatchNormalization()(inception_output_3)\n",
    "    inception_output_3 = Activation(activation)(inception_output_3)\n",
    "    inception_output_3 = Dropout(0.5)(inception_output_3)\n",
    "\n",
    "#     #Block 2\n",
    "#     cnn_2 = Conv2D(50, (10,1), kernel_initializer=init)(cnn_1)\n",
    "#     cnn_2 = BatchNormalization()(cnn_2)\n",
    "#     cnn_2 = Activation(activation)(cnn_2)\n",
    "#     cnn_2 = MaxPooling2D(pool_size=(3, 1), strides=3)(cnn_2)\n",
    "#     cnn_2 = Dropout(0.5)(cnn_2)\n",
    "\n",
    "#     #Block 3\n",
    "#     cnn_3 = Conv2D(100, (10,1), kernel_initializer=init)(cnn_2)\n",
    "#     cnn_3 = BatchNormalization()(cnn_3)\n",
    "#     cnn_3 = Activation(activation)(cnn_3)\n",
    "#     cnn_3 = MaxPooling2D(pool_size=(3, 1), strides=3)(cnn_3)\n",
    "#     cnn_3 = Dropout(0.5)(cnn_3)\n",
    "\n",
    "#     #Block 4\n",
    "#     cnn_4 = Conv2D(200, (10,1), kernel_initializer=init)(cnn_3)\n",
    "#     cnn_4 = BatchNormalization()(cnn_4)\n",
    "#     cnn_4 = Activation(activation)(cnn_4)\n",
    "#     cnn_4 = MaxPooling2D(pool_size=(3, 1), strides=3)(cnn_4)\n",
    "#     cnn_4 = Dropout(0.5)(cnn_4)\n",
    "    cnn_4 = Reshape((-1, 50*3))(inception_output_3)\n",
    "\n",
    "    #RNN Block\n",
    "    gru_1 = GRU(200,return_sequences=True,dropout=0.1)(cnn_4)\n",
    "    gru_2 = GRU(200,return_sequences=True,dropout=0.1)(gru_1)\n",
    "    gru_output_1 = keras.layers.concatenate([gru_1, gru_2], axis = 2)\n",
    "\n",
    "    gru_3 = GRU(200,return_sequences=True,dropout=0.1)(gru_output_1)\n",
    "    gru_output_2 = keras.layers.concatenate([gru_1, gru_2, gru_3], axis = 2)\n",
    "    gru_4 = GRU(200,dropout=0.1)(gru_output_2)\n",
    "    \n",
    "#     gru_1 = GRU(200,return_sequences=True,dropout=0.1)(cnn_4)\n",
    "#     gru_2 = GRU(200,return_sequences=True,dropout=0.1)(gru_1)\n",
    "#     gru_3 = GRU(200,return_sequences=True,dropout=0.1)(gru_2)\n",
    "#     gru_4 = GRU(200,dropout=0.1)(gru_3)\n",
    "    \n",
    "    out = Dense(units=4, kernel_initializer=init)(gru_4)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('softmax')(out)\n",
    "    model = Model(inputs = input_shape, outputs = out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: input_1\n",
      "(None, 1000, 22, 1)\n",
      "(None, 1000, 22, 1)\n",
      "\n",
      "Layer 2: conv2d_1\n",
      "<keras.initializers.VarianceScaling object at 0x0000020404CB55F8>\n",
      "(None, 1000, 22, 1)\n",
      "(None, 200, 1, 50)\n",
      "\n",
      "Layer 3: conv2d_2\n",
      "<keras.initializers.VarianceScaling object at 0x0000020407D31F60>\n",
      "(None, 1000, 22, 1)\n",
      "(None, 200, 1, 50)\n",
      "\n",
      "Layer 4: conv2d_3\n",
      "<keras.initializers.VarianceScaling object at 0x0000020407D59E48>\n",
      "(None, 1000, 22, 1)\n",
      "(None, 200, 1, 50)\n",
      "\n",
      "Layer 5: concatenate_1\n",
      "[(None, 200, 1, 50), (None, 200, 1, 50), (None, 200, 1, 50)]\n",
      "(None, 200, 1, 150)\n",
      "\n",
      "Layer 6: batch_normalization_1\n",
      "(None, 200, 1, 150)\n",
      "(None, 200, 1, 150)\n",
      "\n",
      "Layer 7: activation_1\n",
      "(None, 200, 1, 150)\n",
      "(None, 200, 1, 150)\n",
      "\n",
      "Layer 8: dropout_1\n",
      "(None, 200, 1, 150)\n",
      "(None, 200, 1, 150)\n",
      "\n",
      "Layer 9: conv2d_4\n",
      "<keras.initializers.VarianceScaling object at 0x0000020407D859E8>\n",
      "(None, 200, 1, 150)\n",
      "(None, 40, 1, 50)\n",
      "\n",
      "Layer 10: conv2d_5\n",
      "<keras.initializers.VarianceScaling object at 0x0000020407FA5908>\n",
      "(None, 200, 1, 150)\n",
      "(None, 40, 1, 50)\n",
      "\n",
      "Layer 11: conv2d_6\n",
      "<keras.initializers.VarianceScaling object at 0x0000020437840AC8>\n",
      "(None, 200, 1, 150)\n",
      "(None, 40, 1, 50)\n",
      "\n",
      "Layer 12: concatenate_2\n",
      "[(None, 40, 1, 50), (None, 40, 1, 50), (None, 40, 1, 50)]\n",
      "(None, 40, 1, 150)\n",
      "\n",
      "Layer 13: batch_normalization_2\n",
      "(None, 40, 1, 150)\n",
      "(None, 40, 1, 150)\n",
      "\n",
      "Layer 14: activation_2\n",
      "(None, 40, 1, 150)\n",
      "(None, 40, 1, 150)\n",
      "\n",
      "Layer 15: dropout_2\n",
      "(None, 40, 1, 150)\n",
      "(None, 40, 1, 150)\n",
      "\n",
      "Layer 16: conv2d_7\n",
      "<keras.initializers.VarianceScaling object at 0x0000020437864240>\n",
      "(None, 40, 1, 150)\n",
      "(None, 8, 1, 50)\n",
      "\n",
      "Layer 17: conv2d_8\n",
      "<keras.initializers.VarianceScaling object at 0x00000204378907F0>\n",
      "(None, 40, 1, 150)\n",
      "(None, 8, 1, 50)\n",
      "\n",
      "Layer 18: conv2d_9\n",
      "<keras.initializers.VarianceScaling object at 0x000002045403DC18>\n",
      "(None, 40, 1, 150)\n",
      "(None, 8, 1, 50)\n",
      "\n",
      "Layer 19: concatenate_3\n",
      "[(None, 8, 1, 50), (None, 8, 1, 50), (None, 8, 1, 50)]\n",
      "(None, 8, 1, 150)\n",
      "\n",
      "Layer 20: batch_normalization_3\n",
      "(None, 8, 1, 150)\n",
      "(None, 8, 1, 150)\n",
      "\n",
      "Layer 21: activation_3\n",
      "(None, 8, 1, 150)\n",
      "(None, 8, 1, 150)\n",
      "\n",
      "Layer 22: dropout_3\n",
      "(None, 8, 1, 150)\n",
      "(None, 8, 1, 150)\n",
      "\n",
      "Layer 23: reshape_1\n",
      "(None, 8, 1, 150)\n",
      "(None, 8, 150)\n",
      "\n",
      "Layer 24: gru_1\n",
      "<keras.initializers.VarianceScaling object at 0x0000020458D20B38>\n",
      "(None, 8, 150)\n",
      "(None, 8, 200)\n",
      "\n",
      "Layer 25: gru_2\n",
      "<keras.initializers.VarianceScaling object at 0x000002045930AF98>\n",
      "(None, 8, 200)\n",
      "(None, 8, 200)\n",
      "\n",
      "Layer 26: concatenate_4\n",
      "[(None, 8, 200), (None, 8, 200)]\n",
      "(None, 8, 400)\n",
      "\n",
      "Layer 27: gru_3\n",
      "<keras.initializers.VarianceScaling object at 0x0000020459809F98>\n",
      "(None, 8, 400)\n",
      "(None, 8, 200)\n",
      "\n",
      "Layer 28: concatenate_5\n",
      "[(None, 8, 200), (None, 8, 200), (None, 8, 200)]\n",
      "(None, 8, 600)\n",
      "\n",
      "Layer 29: gru_4\n",
      "<keras.initializers.VarianceScaling object at 0x0000020468E56B70>\n",
      "(None, 8, 600)\n",
      "(None, 200)\n",
      "\n",
      "Layer 30: dense_1\n",
      "<keras.initializers.VarianceScaling object at 0x0000020468D70A58>\n",
      "(None, 200)\n",
      "(None, 4)\n",
      "\n",
      "Layer 31: batch_normalization_4\n",
      "(None, 4)\n",
      "(None, 4)\n",
      "\n",
      "Layer 32: activation_4\n",
      "(None, 4)\n",
      "(None, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "model = create_rnn()\n",
    "for layers in model.layers:\n",
    "    print('Layer %d:' % i, layers.name)\n",
    "#     print(layers.__dict__)\n",
    "    if(hasattr(layers,'kernel_initializer')):\n",
    "        print(layers.kernel_initializer)\n",
    "    print(layers.input_shape)\n",
    "    print(layers.output_shape)\n",
    "    print()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2302 samples, validate on 128 samples\n",
      "Epoch 1/60\n",
      "2302/2302 [==============================] - 11s 5ms/step - loss: 1.4322 - acc: 0.2420 - val_loss: 1.3830 - val_acc: 0.2734\n",
      "Epoch 2/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.3816 - acc: 0.2832 - val_loss: 1.3894 - val_acc: 0.2344\n",
      "Epoch 3/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.3736 - acc: 0.3071 - val_loss: 1.3698 - val_acc: 0.3047\n",
      "Epoch 4/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.3578 - acc: 0.3341 - val_loss: 1.3282 - val_acc: 0.3828\n",
      "Epoch 5/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.3502 - acc: 0.3432 - val_loss: 1.4027 - val_acc: 0.2969\n",
      "Epoch 6/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.3561 - acc: 0.3197 - val_loss: 1.3393 - val_acc: 0.3281\n",
      "Epoch 7/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.3248 - acc: 0.3719 - val_loss: 1.2700 - val_acc: 0.4219\n",
      "Epoch 8/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.2916 - acc: 0.4036 - val_loss: 1.3272 - val_acc: 0.3125\n",
      "Epoch 9/60\n",
      "2302/2302 [==============================] - 6s 3ms/step - loss: 1.2861 - acc: 0.3966 - val_loss: 1.3149 - val_acc: 0.3672\n",
      "Epoch 10/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.2432 - acc: 0.4244 - val_loss: 1.1937 - val_acc: 0.4219\n",
      "Epoch 11/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.2199 - acc: 0.4553 - val_loss: 1.2224 - val_acc: 0.4688\n",
      "Epoch 12/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.1977 - acc: 0.4605 - val_loss: 1.1341 - val_acc: 0.4766\n",
      "Epoch 13/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.1384 - acc: 0.5017 - val_loss: 1.1741 - val_acc: 0.4688\n",
      "Epoch 14/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.1357 - acc: 0.4965 - val_loss: 1.2065 - val_acc: 0.4453\n",
      "Epoch 15/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.1157 - acc: 0.5156 - val_loss: 1.1992 - val_acc: 0.4766\n",
      "Epoch 16/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.0754 - acc: 0.5287 - val_loss: 1.1233 - val_acc: 0.4844\n",
      "Epoch 17/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.0684 - acc: 0.5387 - val_loss: 1.1378 - val_acc: 0.4766\n",
      "Epoch 18/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.0317 - acc: 0.5717 - val_loss: 1.1973 - val_acc: 0.5078\n",
      "Epoch 19/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 1.0257 - acc: 0.5821 - val_loss: 1.1013 - val_acc: 0.5234\n",
      "Epoch 20/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.9987 - acc: 0.5904 - val_loss: 1.0739 - val_acc: 0.5234\n",
      "Epoch 21/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.9668 - acc: 0.6025 - val_loss: 1.0701 - val_acc: 0.5391\n",
      "Epoch 22/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.9592 - acc: 0.6073 - val_loss: 1.1868 - val_acc: 0.4688\n",
      "Epoch 23/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.9079 - acc: 0.6360 - val_loss: 1.0778 - val_acc: 0.5234\n",
      "Epoch 24/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.8966 - acc: 0.6442 - val_loss: 1.0821 - val_acc: 0.5469\n",
      "Epoch 25/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.8798 - acc: 0.6481 - val_loss: 1.1225 - val_acc: 0.5078\n",
      "Epoch 26/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.8543 - acc: 0.6738 - val_loss: 1.0563 - val_acc: 0.5547\n",
      "Epoch 27/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.8341 - acc: 0.6807 - val_loss: 1.1287 - val_acc: 0.5547\n",
      "Epoch 28/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.8142 - acc: 0.6759 - val_loss: 1.0252 - val_acc: 0.5859\n",
      "Epoch 29/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.7466 - acc: 0.7189 - val_loss: 1.0893 - val_acc: 0.5547\n",
      "Epoch 30/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.7543 - acc: 0.7050 - val_loss: 1.0458 - val_acc: 0.5625\n",
      "Epoch 31/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.7286 - acc: 0.7211 - val_loss: 1.0886 - val_acc: 0.5781\n",
      "Epoch 32/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.7010 - acc: 0.7402 - val_loss: 1.0536 - val_acc: 0.5547\n",
      "Epoch 33/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.7059 - acc: 0.7402 - val_loss: 1.1070 - val_acc: 0.5469\n",
      "Epoch 34/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.6525 - acc: 0.7676 - val_loss: 1.1456 - val_acc: 0.5703\n",
      "Epoch 35/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.6757 - acc: 0.7546 - val_loss: 1.0673 - val_acc: 0.5625\n",
      "Epoch 36/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.6229 - acc: 0.7798 - val_loss: 1.1642 - val_acc: 0.5391\n",
      "Epoch 37/60\n",
      "2302/2302 [==============================] - 5s 2ms/step - loss: 0.6327 - acc: 0.7702 - val_loss: 1.0426 - val_acc: 0.5469\n",
      "Epoch 38/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.6195 - acc: 0.7719 - val_loss: 1.0592 - val_acc: 0.6016\n",
      "Epoch 39/60\n",
      "2302/2302 [==============================] - 5s 2ms/step - loss: 0.5839 - acc: 0.7932 - val_loss: 1.2270 - val_acc: 0.5625\n",
      "Epoch 40/60\n",
      "2302/2302 [==============================] - 5s 2ms/step - loss: 0.5483 - acc: 0.8058 - val_loss: 1.0514 - val_acc: 0.5859\n",
      "Epoch 41/60\n",
      "2302/2302 [==============================] - 6s 3ms/step - loss: 0.5210 - acc: 0.8241 - val_loss: 1.2201 - val_acc: 0.5859\n",
      "Epoch 42/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.5643 - acc: 0.7902 - val_loss: 0.9837 - val_acc: 0.6172\n",
      "Epoch 43/60\n",
      "2302/2302 [==============================] - 5s 2ms/step - loss: 0.5379 - acc: 0.8097 - val_loss: 1.0901 - val_acc: 0.6641\n",
      "Epoch 44/60\n",
      "2302/2302 [==============================] - 6s 3ms/step - loss: 0.5083 - acc: 0.8106 - val_loss: 1.1404 - val_acc: 0.5781\n",
      "Epoch 45/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4773 - acc: 0.8349 - val_loss: 1.0509 - val_acc: 0.5781\n",
      "Epoch 46/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4696 - acc: 0.8375 - val_loss: 1.1998 - val_acc: 0.5781\n",
      "Epoch 47/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4932 - acc: 0.8288 - val_loss: 1.1146 - val_acc: 0.5859\n",
      "Epoch 48/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4947 - acc: 0.8271 - val_loss: 1.1096 - val_acc: 0.5703\n",
      "Epoch 49/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4703 - acc: 0.8349 - val_loss: 1.0828 - val_acc: 0.6094\n",
      "Epoch 50/60\n",
      "2302/2302 [==============================] - 5s 2ms/step - loss: 0.4723 - acc: 0.8310 - val_loss: 1.2024 - val_acc: 0.5781\n",
      "Epoch 51/60\n",
      "2302/2302 [==============================] - 5s 2ms/step - loss: 0.4593 - acc: 0.8475 - val_loss: 1.1325 - val_acc: 0.5859\n",
      "Epoch 52/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4352 - acc: 0.8488 - val_loss: 1.3318 - val_acc: 0.5469\n",
      "Epoch 53/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4242 - acc: 0.8571 - val_loss: 1.0893 - val_acc: 0.6562\n",
      "Epoch 54/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4188 - acc: 0.8610 - val_loss: 1.0968 - val_acc: 0.6094\n",
      "Epoch 55/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.3869 - acc: 0.8632 - val_loss: 1.1202 - val_acc: 0.6094\n",
      "Epoch 56/60\n",
      "2302/2302 [==============================] - 6s 2ms/step - loss: 0.4199 - acc: 0.8584 - val_loss: 1.1981 - val_acc: 0.6172\n",
      "Epoch 57/60\n",
      "1808/2302 [======================>.......] - ETA: 1s - loss: 0.3737 - acc: 0.8772"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "batch_size = 16\n",
    "epochs = 60\n",
    "opt = keras.optimizers.adam(lr=0.001)\n",
    "results = []\n",
    "\n",
    "model = create_rnn()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "result = model.fit(X_train, y_train,\n",
    "                   batch_size=batch_size,\n",
    "                   validation_data=(X_val, y_val),\n",
    "                   epochs=epochs)\n",
    "results.append(result.history['val_acc'])\n",
    "\n",
    "plt.plot(result.history['acc'])\n",
    "plt.plot(result.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Accuracy\n",
    "test_results = model.predict(x=X_test, batch_size=batch_size)\n",
    "categ_results = np.argmax(test_results,axis=1)\n",
    "categ_test = np.argmax(y_test,axis=1)\n",
    "test_acc = np.sum(categ_results==categ_test)/len(categ_results)\n",
    "print('Training Accuracy: %.3f' % result.history['acc'][-1])\n",
    "print('Validation Accuracy: %.3f' % result.history['val_acc'][-1])\n",
    "print('Testing Accuracy: %.3f' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
